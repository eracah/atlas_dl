{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'racah'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from os import makedirs, mkdir\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import h5py\n",
    "#from helper_fxns import suppress_stdout_stderr\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle(kwargs):\n",
    "    inds = np.arange(kwargs[kwargs.keys()[0]].shape[0])\n",
    "\n",
    "    #shuffle data\n",
    "    rng = np.random.RandomState(7)\n",
    "    rng.shuffle(inds)\n",
    "    return {k:v[inds] for k,v in kwargs.iteritems()}\n",
    "\n",
    "def split_train_val(prop, kwargs):\n",
    "    tr_prop = prop\n",
    "    inds = np.arange(kwargs[kwargs.keys()[0]].shape[0])\n",
    "    #split train, val, test\n",
    "    num_tr_ex = int((tr_prop*len(inds)))\n",
    "    dind = {}\n",
    "    dind[\"tr\"] = inds[:num_tr_ex]\n",
    "    #dind[\"test\"] = inds[num_tr_ex:num_tr_ex + num_val_ex]\n",
    "    \n",
    "    dind[\"val\"] = inds[num_tr_ex:]\n",
    "    \n",
    "    data = {}\n",
    "    for typ, inds in dind.iteritems():\n",
    "        data[typ] = {k:v[inds] for k,v in kwargs.iteritems()}\n",
    "        \n",
    "        \n",
    "    #dict of dicts, where key is tr, val or test and value is dict of x,y,w,psr, etc.\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(x, max_abs=None):\n",
    "    '''a type of sparse preprocessing, which scales everything between -1 and 1 without losing sparsity'''\n",
    "    #only calculate the statistic using training set\n",
    "    if max_abs is None:\n",
    "        max_abs=np.max(np.abs(x))\n",
    "\n",
    "    #then scale all sets\n",
    "    x /= max_abs\n",
    "\n",
    "    return x, max_abs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, \n",
    "                 bg_cfg_file = './config/BgFileListAug16.txt',\n",
    "                 sig_cfg_file='./config/SignalFileListAug16.txt',\n",
    "                 events_fraction=0.1,\n",
    "                 preproc=True,\n",
    "                 tr_prop=0.8,\n",
    "                 seed=3, \n",
    "                 test = False):\n",
    "        \n",
    "        \n",
    "        self.bg_files = bg_cfg_file if isinstance(bg_cfg_file, list) else [bg_cfg_file]\n",
    "        self.sig_files = sig_cfg_file if isinstance(sig_cfg_file, list) else [sig_cfg_file]\n",
    "        self.all_files = self.bg_files + self.sig_files\n",
    "        self.test = test\n",
    "        self.preproc = preproc\n",
    "        self.tr_prop = tr_prop\n",
    "        self.seed = seed\n",
    "        assert events_fraction >= 0 and events_fraction <= 1. , \"whoa events between 0 and 1!\"\n",
    "        self.events_fraction = events_fraction\n",
    "        \n",
    "         \n",
    "          \n",
    "    def vstack_all(self, data):\n",
    "        for k,v in data.iteritems():\n",
    "            data[k] = np.vstack(tuple(v))\n",
    "        return data\n",
    "    \n",
    "    \n",
    " \n",
    "        \n",
    "            \n",
    "    def make_hist(self, d):\n",
    "        \n",
    "        return np.histogram2d(d['clusphi'],d['cluseta'], bins=(self.phi_bins, self.eta_bins),\n",
    "                              weights=d[\"cluse\"], range=[self.phi_range,self.eta_range])[0] \n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "    def _grab_hdf5_events(self,file_, start):\n",
    "        h5f = h5py.File(file_)\n",
    "        print file_\n",
    "        \n",
    "        all_events = h5f[\"all_events\"]\n",
    "        num_events_in_file = all_events[\"hist\"].shape[0]\n",
    "        \n",
    "        num_events = int(np.ceil(self.events_fraction * num_events_in_file))\n",
    "            \n",
    "        arr_slice = slice(0,num_events)\n",
    "        x,w,psr = [np.expand_dims(all_events[k][arr_slice],axis=1) for k in [\"hist\", \"weight\", \"passSR\"]]\n",
    "        filename = os.path.basename(file_)\n",
    "        jz = [int(filename.split(\"JZ\")[-1].split(\".h5\")[0]) if \"jet\" in filename else -1]\n",
    "        rpv1 = [int(filename.split(\".h5\")[0].split(\"_\")[3]) if \"jet\" not in filename else -1]\n",
    "        rpv2 = [int(filename.split(\".h5\")[0].split(\"_\")[4]) if \"jet\" not in filename else -1]\n",
    "        lbls = dict(jz=jz, rpv1=rpv1, rpv2=rpv2)\n",
    "        lbls = {k:np.expand_dims(num_events*v, axis=1) for k,v in lbls.iteritems()}\n",
    "        data =  dict(x=x, w=w, psr=psr)  \n",
    "        data.update(lbls)\n",
    "        return data\n",
    "        \n",
    "    def grab_file(self, file_,files_dict):\n",
    "        file_dict = self._grab_hdf5_events(file_, start=0)\n",
    "        \n",
    "        for k, v in file_dict.iteritems():\n",
    "            files_dict[k].append(v)\n",
    "        \n",
    "        \n",
    "        return files_dict\n",
    "    \n",
    "    \n",
    "    def grab_events(self, file_list, start=0):\n",
    "        files_dict = {k:[] for k in [\"w\", \"x\", \"psr\", \"jz\", \"rpv1\", \"rpv2\"]}\n",
    "        \n",
    "        if len(file_list) > 0:\n",
    "\n",
    "            for file_ in file_list:\n",
    "                files_dict = self.grab_file(file_, files_dict)\n",
    "\n",
    "            files_dict = self.vstack_all(files_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "        return files_dict\n",
    "    \n",
    "    def get_data_block(self):\n",
    "        bg = self.grab_events(self.bg_files)\n",
    "        sig = self.grab_events(self.sig_files)\n",
    "        if len(sig[sig.keys()[0]]) > 0:\n",
    "            if len(bg[bg.keys()[0]]) > 0:\n",
    "                data = {k:np.vstack((bg[k], sig[k])) for k in bg.keys()}\n",
    "            else:\n",
    "                data = sig\n",
    "        elif len(bg[bg.keys()[0]]) > 0:\n",
    "            data = bg\n",
    "        else:\n",
    "            assert False, \"you got no data\"\n",
    "        \n",
    "\n",
    "        \n",
    "        num_data_bg = len(bg[\"w\"])\n",
    "        num_data_sig = len(sig[\"w\"])\n",
    "\n",
    "        \n",
    "        # 1 means signal, 0 means background\n",
    "        data[\"y\"] = np.zeros((num_data_bg + num_data_sig)).astype('int32')\n",
    "\n",
    "        \n",
    "        #make the last half signal label\n",
    "        data[\"y\"][num_data_bg:] = 1\n",
    "   \n",
    "        return data\n",
    "        \n",
    "\n",
    "    def load_data(self):\n",
    "        t = time.time()\n",
    "        data = self.get_data_block()\n",
    "        print time.time() - t\n",
    "        data = shuffle(data)\n",
    "        if not self.test:\n",
    "            data = split_train_val(self.tr_prop, data)\n",
    "        if self.preproc:\n",
    "            data = self.preprocess(data, keys=[\"x\", \"w\"], raw=[\"w\"])\n",
    "\n",
    "               \n",
    "        return data\n",
    "\n",
    "    \n",
    "    def preprocess(self, data, keys=[\"x\"], raw=[\"w\"]):\n",
    "        \n",
    "        if not self.test:\n",
    "            for k in keys:\n",
    "                tr = copy.deepcopy(data[\"tr\"][k])\n",
    "                val = copy.deepcopy(data[\"val\"][k])\n",
    "                if k in raw:\n",
    "                    data[\"tr\"][\"raw_\"+ k] = tr\n",
    "                    data[\"val\"][\"raw_\" + k] = val\n",
    "                data[\"tr\"][k],tm = preprocess(data[\"tr\"][k])\n",
    "                data[\"val\"][k], _ = preprocess(data[\"val\"][k],tm)\n",
    "\n",
    "        else:\n",
    "            for k in keys:\n",
    "                dat = copy.deepcopy(data[k])\n",
    "                if k in raw:\n",
    "                    data[\"raw_\"+ k] = dat\n",
    "                data[k], _  = preprocess(data[k]) \n",
    "            data = {\"test\":data}\n",
    "        return data\n",
    "        \n",
    "    def iterate_data(self, batch_size=128):\n",
    "#         if self.num_each < batch_size / 2:\n",
    "#             batch_size = 2 * self.num_each\n",
    "#         #only support for hdf5\n",
    "#         for i in range(0, self.num_each, batch_size / 2):\n",
    "#             x_bg = self.grab_events(self.bg_files, batch_size / 2, i)\n",
    "#             x_sig = self.grab_events(self.sig_files, batch_size / 2, i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#does same thing as DataLoader, but makes sure train set is all bg\n",
    "class AnomalyLoader(DataLoader):\n",
    "    def __init__(self, **kwargs):\n",
    "        DataLoader.__init__(self, **kwargs)\n",
    "    \n",
    "    \n",
    "    def load_data(self):\n",
    "        data = self.get_data_block()\n",
    "        data = shuffle(data)\n",
    "        if not self.test:\n",
    "            data = self.split_train_val_anom(self.tr_prop, data)\n",
    "        data = self.preprocess(data)\n",
    "        return data\n",
    "\n",
    "                    \n",
    "    def split_train_val_anom(self,prop, data):\n",
    "        tr_prop = prop\n",
    "        \n",
    "        all_inds = np.arange(data[data.keys()[0]].shape[0])\n",
    "        bg_inds = all_inds[data[\"y\"] == 0]\n",
    "        sig_inds = all_inds[data[\"y\"] == 1]\n",
    "        \n",
    "        #split train, val\n",
    "        #only use background for train\n",
    "        num_tr_ex = int((tr_prop*len(bg_inds)))\n",
    "        \n",
    "        dind = {}\n",
    "        dind[\"tr\"] = bg_inds[:num_tr_ex]\n",
    "\n",
    "\n",
    "        dind[\"val\"] = np.concatenate((bg_inds[num_tr_ex:], sig_inds))\n",
    "\n",
    "        final_data = {}\n",
    "        for typ, inds in dind.iteritems():\n",
    "            final_data[typ] = {k:v[inds] for k,v in data.iteritems()}\n",
    "\n",
    "\n",
    "        #dict of dicts, where key is tr, val or test and value is dict of x,y,w,psr, etc.\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test_files(file_path_list, test_prop=0.2):\n",
    "    \n",
    "    def add_to_file(file_name, data_dict):\n",
    "        f = h5py.File(file_name, \"w\")\n",
    "        group = f.create_group(\"all_events\")\n",
    "        for k in data_dict:\n",
    "            group[k] = data_dict[k]\n",
    "        f.close()\n",
    "        \n",
    "    for file_path in file_path_list:\n",
    "        print file_path\n",
    "        h5f = h5py.File(file_path)\n",
    "        all_events = h5f[\"all_events\"]\n",
    "        num_events = all_events[\"hist\"].shape[0]\n",
    "        \n",
    "        num_test = int(test_prop * num_events)\n",
    "        \n",
    "        test_file_name = join(os.path.dirname(file_path),\"test_\" + os.path.basename(file_path))\n",
    "        train_file_name = join(os.path.dirname(file_path),\"train_\" + os.path.basename(file_path))\n",
    "        \n",
    "        inds = np.arange(num_events)\n",
    "        np.random.RandomState(11).shuffle(inds)\n",
    "        raw_data = {k:all_events[k][:] for k in all_events.keys()}\n",
    "        te_data = {k:raw_data[k][inds[:num_test]] for k in all_events.keys()}\n",
    "        tr_data = {k:raw_data[k][inds[num_test:]] for k in all_events.keys()}\n",
    "        add_to_file(test_file_name, te_data)\n",
    "        add_to_file(train_file_name, tr_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_split():\n",
    "    h5_prefix = \"/global/cscratch1/sd/racah/atlas_h5\"\n",
    "    bg_cfg_file=[join(h5_prefix, \"jetjet_JZ%i.h5\"% (i)) for i in range(3,12)]\n",
    "    sig_cfg_file=[join(h5_prefix, \"GG_RPV10_1400_850.h5\")]\n",
    "    file_list = bg_cfg_file + sig_cfg_file\n",
    "    print file_list\n",
    "    #split_train_test_files(file_path_list=file_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ4.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ5.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ6.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ7.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ8.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ9.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ10.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_jetjet_JZ11.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/train_GG_RPV10_1400_850.h5\n",
      "7.36811184883\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #run_split()\n",
    "    h5_prefix = \"/global/cscratch1/sd/racah/atlas_h5\"\n",
    "    \n",
    "\n",
    "    dl = DataLoader(bg_cfg_file=[join(h5_prefix, \"train_jetjet_JZ%i.h5\"% (i)) for i in range(4,12)],\n",
    "                    sig_cfg_file=join(h5_prefix, \"train_GG_RPV10_1400_850.h5\"),\n",
    "               events_fraction=0.005, test=False, preproc=True)\n",
    "\n",
    "    data= dl.load_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
