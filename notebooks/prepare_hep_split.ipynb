{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mlines\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import h5py as h5\n",
    "\n",
    "#sqlite for storing the metadata\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadatadirs=[\"/global/cscratch1/sd/tkurth/atlas_dl/metadata/64x64/create_metadata/output\"]\n",
    "metadataoutputdir=\"/global/cscratch1/sd/tkurth/atlas_dl/metadata/64x64/create_metadata/output_split\"\n",
    "restart=False\n",
    "numnodes=4096\n",
    "#parameters\n",
    "train_fraction=0.75\n",
    "validation_fraction=0.10\n",
    "nsig_augment=1\n",
    "#binning options\n",
    "eta_range = [-5,5]\n",
    "eta_bins = 64\n",
    "phi_range = [-3.1416, 3.1416]\n",
    "phi_bins = 64\n",
    "#jz cut:\n",
    "jzmin=3\n",
    "jzmax=11\n",
    "#SUSY theory:\n",
    "trainselect=[{'mGlu':1400, 'mNeu': 850}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadatafiles=[]\n",
    "for directory in metadatadirs:\n",
    "    #load files\n",
    "    metadatafiles=[directory+'/'+x for x in os.listdir(directory) if x.endswith('.db')]\n",
    "    \n",
    "#read the database-files\n",
    "dflist=[]\n",
    "for mf in metadatafiles:\n",
    "    con = sql.connect(mf)\n",
    "    tmpdf=pd.DataFrame(pd.read_sql(\"SELECT * FROM metadata;\", con))\n",
    "    con.close()\n",
    "    \n",
    "    #clean up\n",
    "    del tmpdf['index']\n",
    "    dflist.append(tmpdf)\n",
    "\n",
    "#concatenate\n",
    "datadf=pd.concat(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe in GB: 7\n"
     ]
    }
   ],
   "source": [
    "print \"Size of dataframe in GB:\",sys.getsizeof(datadf)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split by signal and BG.\n",
      "Determine frequencies.\n"
     ]
    }
   ],
   "source": [
    "print(\"Split by signal and BG.\")\n",
    "\n",
    "#select signal and background configuration\n",
    "siglist=[]\n",
    "for item in trainselect:\n",
    "    siglist.append(datadf[ (datadf['mGlu']==item['mGlu']) & (datadf['mNeu']==item['mNeu']) ])\n",
    "sigdf=pd.concat(siglist)\n",
    "\n",
    "#select background configuration\n",
    "bgdf=datadf[ (datadf['jz']>=jzmin) & (datadf['jz']<=jzmax) ]\n",
    "\n",
    "print(\"Determine frequencies.\")\n",
    "\n",
    "#background:\n",
    "bggroup=bgdf.groupby(['jz','directory'])\n",
    "tmpdf=pd.DataFrame(bggroup['id'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'id':'frequency'},inplace=True)\n",
    "bgdf=bgdf.merge(tmpdf,on=['jz','directory'],how='left')\n",
    "\n",
    "#signal:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu','directory'])\n",
    "tmpdf=pd.DataFrame(siggroup['id'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'id':'frequency'},inplace=True)\n",
    "sigdf=sigdf.merge(tmpdf,on=['mGlu','mNeu','directory'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got the following total frequencies:\n",
      "Training set: #signal =  4278252  #background =  5568964\n",
      "Validation set: #signal =  570433  #background =  742524\n",
      "Test set: #signal =  855652  #background =  1113804\n"
     ]
    }
   ],
   "source": [
    "#do the splitting\n",
    "#group sigdf according to mGlu and mNeu:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "bggroup=bgdf.groupby(['jz'])\n",
    "\n",
    "\n",
    "#training\n",
    "#for signal, group according to masses and take the fraction for every theory:\n",
    "trainsigdf=siggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "trainbgdf=bggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#validation\n",
    "valsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                        :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "valbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                       :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#test\n",
    "testsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "testbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "print \"We got the following total frequencies:\"\n",
    "print \"Training set: #signal = \",trainsigdf.shape[0],\" #background = \",trainbgdf.shape[0]\n",
    "print \"Validation set: #signal = \",valsigdf.shape[0],\" #background = \",valbgdf.shape[0]\n",
    "print \"Test set: #signal = \",testsigdf.shape[0],\" #background = \",testbgdf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into input files for multi-node processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the dataframes:\n",
    "#train\n",
    "traindf=pd.concat([trainsigdf,trainbgdf])\n",
    "traindf.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#validation\n",
    "validf=pd.concat([valsigdf,valbgdf])\n",
    "validf.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#test\n",
    "testdf=pd.concat([testsigdf,testbgdf])\n",
    "testdf.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "#shuffle:\n",
    "#train\n",
    "np.random.seed(13)\n",
    "traindf=traindf.reindex(np.random.permutation(traindf.index))\n",
    "traindf.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#validation\n",
    "np.random.seed(13)\n",
    "validf=validf.reindex(np.random.permutation(validf.index))\n",
    "validf.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#test\n",
    "np.random.seed(13)\n",
    "testdf=testdf.reindex(np.random.permutation(testdf.index))\n",
    "testdf.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  9847216  chunk size:  2404\n",
      "Validation size:  1312957  chunk size:  320\n",
      "Test size:  1969456  chunk size:  480\n"
     ]
    }
   ],
   "source": [
    "#print ensemble sizes and determine the chunk size\n",
    "chunksize_train=int(traindf.shape[0]/float(numnodes))\n",
    "print \"Training size: \",int(traindf.shape[0]),' chunk size: ',chunksize_train\n",
    "chunksize_validation=int(validf.shape[0]/float(numnodes))\n",
    "print \"Validation size: \",validf.shape[0],' chunk size: ',chunksize_validation\n",
    "chunksize_test=int(testdf.shape[0]/float(numnodes))\n",
    "print \"Test size: \",testdf.shape[0],' chunk size: ',chunksize_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store normalizations and throw away stuff which is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize weights\n",
    "maxnormweight=np.max(traindf['weight_max']/traindf['count'])\n",
    "#normweight\n",
    "traindf[\"normweight_norm\"]=1./(traindf['count']*maxnormweight)\n",
    "validf[\"normweight_norm\"]=1./(validf['count']*maxnormweight)\n",
    "testdf[\"normweight_norm\"]=1./(testdf['count']*maxnormweight)\n",
    "#regular weight\n",
    "traindf[\"weight_norm\"]=1./traindf['count']\n",
    "validf[\"weight_norm\"]=1./validf['count']\n",
    "testdf[\"weight_norm\"]=1./testdf['count']\n",
    "#normalize input channels\n",
    "#clusE\n",
    "max_clusE_max=np.max(traindf['clusE_max'])\n",
    "traindf['clusE_norm']=1./max_clusE_max\n",
    "validf['clusE_norm']=1./max_clusE_max\n",
    "testdf['clusE_norm']=1./max_clusE_max\n",
    "#clusEM\n",
    "max_clusEM_max=np.max(traindf['clusEM_max'])\n",
    "traindf['clusEM_norm']=1./max_clusEM_max\n",
    "validf['clusEM_norm']=1./max_clusEM_max\n",
    "testdf['clusEM_norm']=1./max_clusEM_max\n",
    "#track\n",
    "max_track_max=float(np.max(traindf['track_max']))\n",
    "traindf['track_norm']=1./max_track_max\n",
    "validf['track_norm']=1./max_track_max\n",
    "testdf['track_norm']=1./max_track_max\n",
    "\n",
    "#now, set label\n",
    "#train\n",
    "traindf['label']=0\n",
    "traindf.loc[traindf.jz == 0., 'label']=1\n",
    "#validation\n",
    "validf['label']=0\n",
    "validf.loc[validf.jz == 0., 'label']=1\n",
    "#test\n",
    "testdf['label']=0\n",
    "testdf.loc[testdf.jz == 0., 'label']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out splitted dataframes so that individual processes can chew trough the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#see which files are done\n",
    "ind_done={'train':[], 'validation':[], 'test':[]}\n",
    "if not restart:\n",
    "    filelist=[x for x in os.listdir(metadataoutputdir) if x.endswith('.db')]\n",
    "    for fname in filelist:\n",
    "        phasename=fname.split('_')[0]\n",
    "        ind=int(fname.split('_chunk')[1].split('.db')[0])\n",
    "        ind_done[phasename].append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [], 'train': [], 'validation': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training\n",
    "for ind in range(numnodes):\n",
    "    #skip if already done:\n",
    "    if ind in ind_done['train']:\n",
    "        continue\n",
    "    \n",
    "    #upper and lower index\n",
    "    lo=ind*chunksize_train\n",
    "    up=(ind+1)*chunksize_train\n",
    "    \n",
    "    #get slice:\n",
    "    seldf=traindf[['directory','filename','id','label','normweight_norm','weight_norm','clusE_norm','clusEM_norm','track_norm','mGlu','mNeu','jz']].iloc[lo:up,:]\n",
    "    \n",
    "    #establish db connection\n",
    "    con = sql.connect(metadataoutputdir+'/train_metadata_chunk'+str(ind)+'.db')\n",
    "    \n",
    "    #write out:\n",
    "    seldf.to_sql(\"metadata\", con, if_exists='replace',chunksize=200)\n",
    "    \n",
    "    #close connection:\n",
    "    con.close()\n",
    "\n",
    "#validation\n",
    "for ind in range(numnodes):\n",
    "    #skip if already done:\n",
    "    if ind in ind_done['validation']:\n",
    "        continue\n",
    "    \n",
    "    #upper and lower index\n",
    "    lo=ind*chunksize_validation\n",
    "    up=(ind+1)*chunksize_validation\n",
    "    \n",
    "    #get slice:\n",
    "    seldf=validf[['directory','filename','id','label','normweight_norm','weight_norm','clusE_norm','clusEM_norm','track_norm','mGlu','mNeu','jz']].iloc[lo:up,:]\n",
    "    \n",
    "    #establish db connection\n",
    "    con = sql.connect(metadataoutputdir+'/validation_metadata_chunk'+str(ind)+'.db')\n",
    "    \n",
    "    #write out:\n",
    "    seldf.to_sql(\"metadata\", con, if_exists='replace',chunksize=200)\n",
    "    \n",
    "    #close connection:\n",
    "    con.close()\n",
    "\n",
    "#test\n",
    "for ind in range(numnodes):\n",
    "    #skip if already done:\n",
    "    if ind in ind_done['test']:\n",
    "        continue\n",
    "    \n",
    "    #upper and lower index\n",
    "    lo=ind*chunksize_test\n",
    "    up=(ind+1)*chunksize_test\n",
    "    \n",
    "    #get slice:\n",
    "    seldf=testdf[['directory','filename','id','label','normweight_norm','weight_norm','clusE_norm','clusEM_norm','track_norm','mGlu','mNeu','jz']].iloc[lo:up,:]\n",
    "    \n",
    "    #establish db connection\n",
    "    con = sql.connect(metadataoutputdir+'/test_metadata_chunk'+str(ind)+'.db')\n",
    "    \n",
    "    #write out:\n",
    "    seldf.to_sql(\"metadata\", con, if_exists='replace',chunksize=200)\n",
    "    \n",
    "    #close connection:\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "thorstendl",
   "language": "python",
   "name": "thorstendl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
