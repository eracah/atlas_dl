{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os stuff\n",
    "import os\n",
    "import h5py as h5\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "#sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as tfk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \n",
    "    def reset(self):\n",
    "        self._epochs_completed = 0\n",
    "        self._file_index = 0\n",
    "        self._data_index = 0\n",
    "    \n",
    "    \n",
    "    def load_next_file(self):\n",
    "        with h5.File(self._filelist[self._file_index],'r') as f:\n",
    "            self._images = f['data'].value\n",
    "            self._labels = f['label'].value\n",
    "            self._normweights = f['normweight'].value\n",
    "            self._weights = f['weight'].value\n",
    "            f.close()\n",
    "        assert self._images.shape[0] == self._labels.shape[0], ('images.shape: %s labels.shape: %s' % (self._images.shape, self_.labels.shape))\n",
    "        assert self._labels.shape[0] == self._normweights.shape[0], ('labels.shape: %s normweights.shape: %s' % (self._labels.shape, self._normweights.shape))\n",
    "        \n",
    "        #set number of samples\n",
    "        self._num_examples = self._labels.shape[0]\n",
    "        \n",
    "        #create permutation\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        \n",
    "        #shuffle\n",
    "        self._images = self._images[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        self._normweights = self._normweights[perm]\n",
    "        self._weights = self._weights[perm]\n",
    "        \n",
    "        #transpose images\n",
    "        self._images = np.transpose(self._images,(0,3,2,1))\n",
    "        #select one channel only\n",
    "        self._images = self._images[:,:,:,0:1]\n",
    "        \n",
    "        #reshape labels and weights\n",
    "        self._labels = np.reshape(self._labels,(self._labels.shape[0],1))\n",
    "        self._normweights = np.reshape(self._normweights,(self._normweights.shape[0],1))\n",
    "        self._weights = np.reshape(self._weights,(self._weights.shape[0],1))\n",
    "        \n",
    "    \n",
    "    def __init__(self, filelist):\n",
    "        \"\"\"Construct DataSet\"\"\"\n",
    "        self._num_files = len(filelist)\n",
    "        \n",
    "        assert self._num_files > 0, ('filelist is empty')\n",
    "        \n",
    "        self._filelist = filelist\n",
    "        self.reset()\n",
    "        self.load_next_file()\n",
    "\n",
    "    @property\n",
    "    def num_files(self):\n",
    "        return self._num_files\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._data_index\n",
    "        self._data_index += batch_size\n",
    "        if self._data_index > self._num_examples:\n",
    "            \n",
    "            #first, reset data_index and increase file index:\n",
    "            start=0\n",
    "            self._data_index=batch_size\n",
    "            self._file_index+=1\n",
    "            \n",
    "            #check if we are at the end of the file list\n",
    "            if self._file_index >= self._num_files:\n",
    "                #epoch is finished\n",
    "                self._epochs_completed += 1\n",
    "                #reset file index and shuffle list\n",
    "                self._file_index=0\n",
    "                np.random.shuffle(self._filelist)\n",
    "            \n",
    "            #load the next file\n",
    "            self.load_next_file()\n",
    "            assert batch_size <= self._num_examples\n",
    "        \n",
    "        end = self._data_index\n",
    "        return self._images[start:end], self._labels[start:end], self._normweights[start:end], self._weights[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(args):\n",
    "    \n",
    "    #define empty variables dict\n",
    "    variables={}\n",
    "    \n",
    "    #create placeholders\n",
    "    variables['images_'] = tf.placeholder(tf.float32, shape=args['input_shape'])\n",
    "    variables['keep_prob_'] = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #empty network:\n",
    "    network = []\n",
    "    \n",
    "    #input layer\n",
    "    network.append(tf.reshape(variables['images_'], [-1]+args['input_shape'][1:], name='input'))\n",
    "    \n",
    "    #get all the conv-args stuff:\n",
    "    activation=args['conv_params']['activation']\n",
    "    initializer=args['conv_params']['initializer']\n",
    "    ksize=args['conv_params']['filter_size']\n",
    "    num_filters=args['conv_params']['num_filters']\n",
    "    padding=args['conv_params']['padding']\n",
    "        \n",
    "    #conv layers:\n",
    "    prev_num_filters=1\n",
    "    for layerid in range(1,args['num_layers']+1):\n",
    "        \n",
    "        #create weight-variable\n",
    "        variables['conv'+str(layerid)+'_w']=tf.Variable(initializer([ksize,ksize,prev_num_filters,num_filters]),\n",
    "                                                        name='conv'+str(layerid)+'_w')\n",
    "        prev_num_filters=num_filters\n",
    "        \n",
    "        #conv unit\n",
    "        network.append(tf.nn.conv2d(network[-1],\n",
    "                                    filter=variables['conv'+str(layerid)+'_w'],\n",
    "                                    strides=[1, 1, 1, 1], \n",
    "                                    padding=padding, \n",
    "                                    name='conv'+str(layerid)))\n",
    "        \n",
    "        outshape=network[-1].shape[1:]\n",
    "        if args['batch_norm']:\n",
    "            #mu\n",
    "            variables['bn'+str(layerid)+'_m']=tf.Variable(tf.zeros(outshape),\n",
    "                                                         name='bn'+str(layerid)+'_m')\n",
    "            #sigma\n",
    "            variables['bn'+str(layerid)+'_s']=tf.Variable(tf.ones(outshape),\n",
    "                                                         name='bn'+str(layerid)+'_s')\n",
    "            #gamma\n",
    "            variables['bn'+str(layerid)+'_g']=tf.Variable(tf.ones(outshape),\n",
    "                                                         name='bn'+str(layerid)+'_g')\n",
    "            #beta\n",
    "            variables['bn'+str(layerid)+'_b']=tf.Variable(tf.zeros(outshape),\n",
    "                                                         name='bn'+str(layerid)+'_b')\n",
    "            #add batch norm layer\n",
    "            network.append(tf.nn.batch_normalization(network[-1],\n",
    "                           mean=variables['bn'+str(layerid)+'_m'],\n",
    "                           variance=variables['bn'+str(layerid)+'_s'],\n",
    "                           offset=variables['bn'+str(layerid)+'_b'],\n",
    "                           scale=variables['bn'+str(layerid)+'_g'],\n",
    "                           variance_epsilon=1.e-4,\n",
    "                           name='bn'+str(layerid)))\n",
    "        \n",
    "        #add relu unit:\n",
    "        network.append(activation(network[-1]))\n",
    "        \n",
    "        #add dropout\n",
    "        network.append(tf.nn.dropout(network[-1],\n",
    "                                     keep_prob=variables['keep_prob_'],\n",
    "                                     name='drop'+str(layerid)))\n",
    "        \n",
    "        #add maxpool\n",
    "        network.append(tf.nn.max_pool(network[-1],\n",
    "                                      ksize=[1,2,2,1],\n",
    "                                      strides=[1,2,2,1],\n",
    "                                      padding=args['conv_params']['padding'],\n",
    "                                      name='maxpool'+str(layerid)))\n",
    "    \n",
    "    #reshape\n",
    "    network.append(tf.reshape(network[-1],shape=[-1, 8 * 8 * num_filters],name='flatten'))\n",
    "    \n",
    "    #now do the MLP\n",
    "    #fc1\n",
    "    variables['fc1_w']=tf.Variable(initializer([8 * 8 * num_filters,args['num_fc_units']]),name='fc1_w')\n",
    "    variables['fc1_b']=tf.Variable(tf.zeros([args['num_fc_units']]),name='fc1_b')\n",
    "    network.append(tf.matmul(network[-1], variables['fc1_w']) + variables['fc1_b'])\n",
    "    \n",
    "    #dropout\n",
    "    network.append(tf.nn.dropout(network[-1],\n",
    "                                     keep_prob=variables['keep_prob_'],\n",
    "                                     name='drop'+str(layerid)))\n",
    "    #fc2\n",
    "    variables['fc2_w']=tf.Variable(initializer([args['num_fc_units'],2]),name='fc2_w')\n",
    "    variables['fc2_b']=tf.Variable(tf.zeros([2]),name='fc2_b')\n",
    "    network.append(tf.matmul(network[-1], variables['fc2_w']) + variables['fc2_b'])\n",
    "    \n",
    "    #softmax\n",
    "    network.append(tf.nn.softmax(network[-1]))\n",
    "    \n",
    "    #return the network and variables\n",
    "    return variables,network\n",
    "\n",
    "\n",
    "#build the functions\n",
    "def build_functions(variables, network):\n",
    "    #add additional variables\n",
    "    variables['labels_']=tf.placeholder(tf.int32,shape=[None,1])\n",
    "    variables['weights_']=tf.placeholder(tf.float32,shape=[None,1])\n",
    "    \n",
    "    #loss function\n",
    "    prediction = network[-1]\n",
    "    \n",
    "    #compute loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(variables['labels_'],\n",
    "                                                  prediction,\n",
    "                                                  weights=variables['weights_'])\n",
    "    \n",
    "    #compute accuracy\n",
    "    accuracy = tf.metrics.accuracy(variables['labels_'],\n",
    "                                   tf.round(prediction[:,1]),\n",
    "                                   weights=variables['weights_'],\n",
    "                                   name='accuracy')\n",
    "    \n",
    "    #compute AUC\n",
    "    auc = tf.metrics.auc(variables['labels_'],\n",
    "                         prediction[:,1],\n",
    "                         weights=variables['weights_'],\n",
    "                         num_thresholds=5000,\n",
    "                         curve='ROC',\n",
    "                         name='AUC')\n",
    "    \n",
    "    #get loss\n",
    "    return prediction, loss, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args={'input_shape': [None, 64, 64, 1], \n",
    "                      'save_interval': 5,\n",
    "                      'learning_rate': 1.e-6, \n",
    "                      'dropout_p': 0.5, \n",
    "                      'weight_decay': 0, #0.0001, \n",
    "                      'num_fc_units': 512,\n",
    "                      'num_layers': 3,\n",
    "                      'momentum': 0.9,\n",
    "                      'num_epochs': 200,\n",
    "                      'train_batch_size': 512, #480\n",
    "                      'validation_batch_size': 320, #480\n",
    "                      'batch_norm': True,\n",
    "                      'conv_params': dict(num_filters=128, \n",
    "                                       filter_size=3, padding='SAME', \n",
    "                                       activation=tf.nn.relu, \n",
    "                                       initializer=tfk.initializers.he_normal())\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables, network = build_cnn_model(args)\n",
    "pred_fn, loss_fn, accuracy_fn, auc_fn = build_functions(variables, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path\n",
    "inputpath = '/global/cscratch1/sd/tkurth/atlas_dl/data_delphes_final_64x64'\n",
    "logpath = './tensorflow_logs'\n",
    "modelpath = './tensorflow_models'\n",
    "#training files\n",
    "trainfiles = [inputpath+'/'+x for x in os.listdir(inputpath) if x.startswith('hep_train') and x.endswith('.hdf5')]\n",
    "trainset=DataSet(trainfiles[0:20])\n",
    "#validation files\n",
    "validationfiles = [inputpath+'/'+x for x in os.listdir(inputpath) if x.startswith('hep_valid') and x.endswith('.hdf5')]\n",
    "validationset=DataSet(validationfiles[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #train on training loss\n",
    "    train_step = tf.train.AdamOptimizer(args['learning_rate']).minimize(loss_fn)\n",
    "\n",
    "    #create summaries\n",
    "    var_summary = []\n",
    "    for item in variables:\n",
    "        var_summary.append(tf.summary.histogram(item,variables[item]))\n",
    "        #if item.startswith('conv'):\n",
    "        #    #add additional image feature maps\n",
    "        #    for i in range(variables_dict.shape[])\n",
    "        #    tf.summary.image()\n",
    "    summary_loss = tf.summary.scalar(\"loss\",loss_fn)\n",
    "    summary_accuracy = tf.summary.scalar(\"accuracy\",accuracy_fn)\n",
    "    train_summary = tf.summary.merge([summary_loss]+var_summary)\n",
    "    validation_summary = tf.summary.merge([summary_loss])\n",
    "    train_writer = tf.summary.FileWriter(logpath+'/hep_classifier_log', sess.graph)\n",
    "    \n",
    "    # Add an op to initialize the variables.\n",
    "    init_global_op = tf.global_variables_initializer()\n",
    "    init_local_op = tf.local_variables_initializer()\n",
    "    \n",
    "    #saver class:\n",
    "    model_saver = tf.train.Saver()\n",
    "    \n",
    "    #initialize variables\n",
    "    sess.run([init_global_op,init_local_op])\n",
    "    \n",
    "    #counter stuff\n",
    "    epochs_completed=0\n",
    "    trainset.reset()\n",
    "    validationset.reset()\n",
    "    train_loss=0.\n",
    "    train_batches=0\n",
    "    total_batches=0\n",
    "    \n",
    "    #do training\n",
    "    while epochs_completed < args['num_epochs']:\n",
    "        \n",
    "        #increment total batch counter\n",
    "        total_batches+=1\n",
    "        \n",
    "        #get next batch\n",
    "        images,labels,normweights,_ = trainset.next_batch(args['train_batch_size'])  \n",
    "    \n",
    "        #update weights\n",
    "        _, summary, tmp_loss = sess.run([train_step, train_summary, loss_fn],\n",
    "                                           feed_dict={variables['images_']: images, \n",
    "                                              variables['labels_']: labels, \n",
    "                                              variables['weights_']: normweights, \n",
    "                                              variables['keep_prob_']: args['dropout_p']})\n",
    "        \n",
    "        #add to summary\n",
    "        train_writer.add_summary(summary, total_batches)\n",
    "        \n",
    "        #increment train loss and batch number\n",
    "        train_loss += tmp_loss\n",
    "        train_batches += 1\n",
    "\n",
    "        #check if epoch is done\n",
    "        if trainset._epochs_completed>epochs_completed:\n",
    "            epochs_completed=trainset._epochs_completed\n",
    "            print(\"epoch %d, average training loss %g\"%(epochs_completed, train_loss/float(train_batches)))\n",
    "            train_loss=0.\n",
    "            train_batches=0\n",
    "            \n",
    "            #compute validation loss:\n",
    "            #reset variables\n",
    "            validation_loss=0.\n",
    "            validation_batches=0\n",
    "            sess.run(init_local_op)\n",
    "            \n",
    "            all_labels=[]\n",
    "            all_weights=[]\n",
    "            all_pred=[]\n",
    "            \n",
    "            #iterate over batches\n",
    "            while True:\n",
    "                #get next batch\n",
    "                images,labels,normweights,weights = validationset.next_batch(args['validation_batch_size'])\n",
    "                #compute loss\n",
    "                summary, tmp_loss=sess.run([validation_summary,loss_fn],\n",
    "                                            feed_dict={variables['images_']: images, \n",
    "                                                        variables['labels_']: labels, \n",
    "                                                        variables['weights_']: normweights, \n",
    "                                                        variables['keep_prob_']: 1.0})\n",
    "                \n",
    "                #add loss\n",
    "                validation_loss += tmp_loss\n",
    "                validation_batches += 1\n",
    "                \n",
    "                #update accuracy\n",
    "                sess.run(accuracy_fn[1],feed_dict={variables['images_']: images, \n",
    "                                                    variables['labels_']: labels, \n",
    "                                                    variables['weights_']: weights, \n",
    "                                                    variables['keep_prob_']: 1.0})\n",
    "                \n",
    "                #update auc\n",
    "                sess.run(auc_fn[1],feed_dict={variables['images_']: images, \n",
    "                                              variables['labels_']: labels, \n",
    "                                              variables['weights_']: weights, \n",
    "                                              variables['keep_prob_']: 1.0})\n",
    "                \n",
    "                #debugging\n",
    "                #pred = sess.run(pred_fn,\n",
    "                #                feed_dict={variables['images_']: images, \n",
    "                #                            variables['labels_']: labels, \n",
    "                #                            variables['weights_']: weights, \n",
    "                #                            variables['keep_prob_']: 1.0})\n",
    "                #all_labels.append(labels)\n",
    "                #all_weights.append(weights)\n",
    "                #all_pred.append(pred[:,1])\n",
    "                \n",
    "                #check if full pass done\n",
    "                if validationset._epochs_completed>0:\n",
    "                    validationset.reset()\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            #sklearn ROC\n",
    "            #all_labels = np.concatenate(all_labels,axis=0).flatten()\n",
    "            #all_pred = np.concatenate(all_pred,axis=0).flatten()\n",
    "            #all_weights = np.concatenate(all_weights,axis=0).flatten()\n",
    "            #fpr, tpr, thresholds = metrics.roc_curve(all_labels, all_pred, pos_label=1, sample_weight=all_weights)\n",
    "            #print(\"epoch %d, sklearn AUC %g\"%(epochs_completed,metrics.auc(fpr,tpr,reorder=True)))\n",
    "            \n",
    "            print(\"epoch %d, average validation loss %g\"%(epochs_completed, validation_loss/float(validation_batches)))\n",
    "            validation_accuracy = sess.run(accuracy_fn[0])\n",
    "            print(\"epoch %d, average validation accu %g\"%(epochs_completed, validation_accuracy))\n",
    "            validation_auc = sess.run(auc_fn[0])\n",
    "            print(\"epoch %d, average validation auc %g\"%(epochs_completed, validation_auc))\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epochs_completed%args['save_interval']==0:\n",
    "                model_save_path = model_saver.save(sess, modelpath+'/hep_classifier_tfmodel_epoch_'+str(epochs_completed)+'.ckpt')\n",
    "                print 'Model saved in file: %s'%model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "thorstendl",
   "language": "python",
   "name": "thorstendl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
