{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'notebooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a194ec434b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_caffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_caffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinary_classifier_caffe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'notebooks'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import argparse\n",
    "from os.path import join\n",
    "import notebooks.load_data_caffe.data_loader as dl\n",
    "from notebooks.load_data_caffe.data_loader import DataIterator\n",
    "from notebooks.networks import binary_classifier_caffe as bc\n",
    "#from notebooks.networks import anom_ae as aa\n",
    "from notebooks.util import create_run_dir, get_logger, dump_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_args = {'input_shape': tuple([None] + [3, 224, 224]), \n",
    "                      'learning_rate': 0.00001, \n",
    "                      'dropout_p': 0.5,\n",
    "                      'leakiness': 0.1,\n",
    "                      'weight_decay': 0.0,\n",
    "                      'num_filters': 128, \n",
    "                      'num_fc_units': 1024,\n",
    "                      'num_layers': 4,\n",
    "                      'momentum': 0.9,\n",
    "                      'num_epochs': 20000,\n",
    "                      'batch_size': 128,\n",
    "                      \"save_path\": \"None\",\n",
    "                      \"num_tr\": -1,\n",
    "                      \"test\":False, \n",
    "                      \"seed\": 7,\n",
    "                      \"mode\":\"classif\",\n",
    "                      \"exp_name\": \"run\",\n",
    "                      \"load_path\": \"None\",\n",
    "                      \"num_test\": -1,\n",
    "                      \"batch_norm\": False\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_configs():\n",
    "    \n",
    "\n",
    "    \n",
    "    # if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "    if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "        sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    #make a command line argument for every flag in default args\n",
    "    for k,v in default_args.iteritems():\n",
    "        if type(v) is bool:\n",
    "            parser.add_argument('--' + k, action='store_true', help=k)\n",
    "        else:\n",
    "            parser.add_argument('--' + k, type=type(v), default=v, help=k)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    kwargs = default_args\n",
    "    kwargs.update(args.__dict__)\n",
    "    \n",
    "    \n",
    "    kwargs = setup_res_dir(kwargs)\n",
    "    \n",
    "    kwargs = setup_iterators(kwargs)\n",
    "\n",
    "    kwargs[\"logger\"] = get_logger(kwargs['save_path'])\n",
    "    \n",
    "    if kwargs[\"ae\"]:\n",
    "        net = aa\n",
    "    else:\n",
    "        net = bc\n",
    "        \n",
    "    kwargs[\"net\"] = net\n",
    "\n",
    "\n",
    "    #kwargs[\"num_train\"], kwargs[\"num_val\"] = trdi.hgroup[\"hist\"].shape[0], valdi.hgroup[\"hist\"].shape[0]\n",
    "    kwargs[\"logger\"].info(str(kwargs))\n",
    "    \n",
    "    dump_hyperparams(dic=kwargs,path=kwargs[\"save_path\"])\n",
    "\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_iterators(kwargs):\n",
    "    loader_kwargs = dict(batch_size=kwargs[\"batch_size\"],\n",
    "                         trainfiles=dl.trainfiles,\n",
    "                         validationfiles=dl.validationfiles,\n",
    "                         testfiles=dl.testfiles,\n",
    "                         keys={\"datakey\": \"data\", \"labelkey\": \"label\", \"normweightkey\":\"normweight\", \"weightkey\":\"weight\"})\n",
    "    kwargs[\"loader_kwargs\"] = loader_kwargs\n",
    "    \n",
    "    if not kwargs[\"test\"]:\n",
    "        #training\n",
    "        trdi = DataIterator(kwargs[\"trainfiles\"], batch_size=kwargs[\"batch_size\"], keys=loader_kwargs['keys'])\n",
    "        kwargs[\"tr_iterator\"] = trdi\n",
    "        kwargs[\"num_tr\"] = trdi.num_events\n",
    "        #validation\n",
    "        valdi = DataIterator(kwargs[\"validationfiles\"], batch_size=kwargs[\"batch_size\"], keys=loader_kwargs['keys'])\n",
    "        kwargs[\"val_iterator\"] = valdi\n",
    "        kwargs[\"num_val\"] = valdi.num_events\n",
    "        \n",
    "        #shape\n",
    "        kwargs[\"input_shape\"] = tuple([None] + list(trdi.data.shape[1:]))\n",
    "    \n",
    "    else:\n",
    "        #test\n",
    "        tsdi = DataIterator(kwargs[\"testfiles\"], batch_size=kwargs[\"batch_size\"], keys=loader_kwargs['keys'])\n",
    "        kwargs[\"test_iterator\"] = tsdi\n",
    "        kwargs[\"num_test\"] = tsdi.num_events\n",
    "        \n",
    "        #shape\n",
    "        kwargs[\"input_shape\"] = tuple([None] + list(tsdi.data.shape[1:]))\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_res_dir(kwargs):\n",
    "    if kwargs[\"save_path\"]== \"None\":\n",
    "        kwargs[\"save_path\"] = None\n",
    "\n",
    "    run_dir = create_run_dir(kwargs[\"save_path\"], name=kwargs[\"exp_name\"])\n",
    "    kwargs['save_path'] = run_dir\n",
    "    return kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
