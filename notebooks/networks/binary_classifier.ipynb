{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "from lasagne.updates import *\n",
    "from lasagne.init import *\n",
    "from lasagne.nonlinearities import rectify as relu\n",
    "from lasagne.nonlinearities import *\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import sys\n",
    "import numpy as np\n",
    "#enable importing of notebooks\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from helper_fxns import get_best_box, get_detec_loss, get_iou, make_test_data, get_detec_acc, get_final_box\n",
    "# if __name__ == \"__main__\":\n",
    "#     from data_loader import load_classification_dataset, load_detection_dataset\n",
    "\n",
    "def build_network(args, network):\n",
    "    X = T.tensor4('X')\n",
    "    Y = T.ivector('Y')\n",
    "    \n",
    "    #physics weights\n",
    "    W = T.dvector('W')\n",
    "    \n",
    "    #make sum to 1\n",
    "    #w = W / T.sum(W)\n",
    "    #network = build_layers(args)\n",
    "    \n",
    "    '''write loss function equation'''\n",
    "    prediction = get_output(network, X)\n",
    "    loss = categorical_crossentropy(prediction, Y)\n",
    "    \n",
    "    #multiply by weights\n",
    "    loss =  T.dot(loss.T,W)\n",
    "    weightsl2 = regularize_network_params(network, l2)\n",
    "    loss += args['weight_decay'] * weightsl2\n",
    "    \n",
    "    '''calculate test loss (cross entropy with no regularization) and accuracy'''\n",
    "    test_prediction = get_output(network, X, deterministic=True)\n",
    "    test_loss = categorical_crossentropy(test_prediction, Y)\n",
    "    test_loss = T.dot(test_loss.T,W)\n",
    "    \n",
    "    \n",
    "    '''classification percentage: we can change this based on false postive/false negative criteria'''\n",
    "    test_acc = categorical_accuracy(test_prediction,Y)\n",
    "    test_acc = T.dot(test_acc.T,W) / T.sum(W)\n",
    "    params = get_all_params(network, trainable=True)\n",
    "    \n",
    "    updates = adam(loss, learning_rate=args['learning_rate'], params=params)\n",
    "    #updates = nesterov_momentum(loss, params, learning_rate=args['learning_rate'], momentum=args['momentum'])\n",
    "    \n",
    "    \n",
    "    '''train_fn -> takes in input,label pairs -> outputs loss '''\n",
    "    train_fn = theano.function([X, Y, W], loss, updates=updates)\n",
    "    \n",
    "    \n",
    "    '''val_fn -> takes in input,label pairs -> outputs non regularized loss and accuracy '''\n",
    "    val_fn = theano.function([X, Y, W], test_loss)\n",
    "    acc_fn = theano.function([X, Y, W], test_acc)\n",
    "    out_fn = theano.function([X], test_prediction)\n",
    "    score_fn = theano.function([X], test_prediction[:,1].T)\n",
    "    return {\"net\":network}, {'tr': train_fn, \n",
    "                            'val': val_fn,\n",
    "                            'acc': acc_fn,\n",
    "                            'out': out_fn, \"score\":score_fn}\n",
    "\n",
    "def build_layers(args):\n",
    "    \n",
    "    conv_kwargs = dict(num_filters=args['num_filters'], filter_size=3, pad=1, nonlinearity=relu, W=HeNormal(gain=\"relu\"))\n",
    "    network = InputLayer(shape=args['input_shape'])\n",
    "    for lay in range(args['num_layers']):\n",
    "        network = batch_norm(Conv2DLayer(network, **conv_kwargs))\n",
    "        network = MaxPool2DLayer(network, pool_size=(2,2),stride=2)\n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network,num_units=args['num_fc_units'], nonlinearity=relu) \n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network, num_units=2, nonlinearity=softmax)\n",
    "    \n",
    "    for layer in get_all_layers(network):\n",
    "        if \"logger\" in args:\n",
    "            args[\"logger\"].info(str(layer) + str(layer.output_shape))\n",
    "    print count_params(layer)\n",
    "    \n",
    "    return network\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# def auc(pred,gt):\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22588\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inp_d = {'input_shape': tuple([None] + [1, 64, 64]), \n",
    "                      'learning_rate': 0.01, \n",
    "                      'dropout_p': 0.5, \n",
    "                      'weight_decay': 0, #0.0001, \n",
    "                      'num_filters': 10, \n",
    "                      'num_fc_units': 32,\n",
    "                      'num_layers': 3,\n",
    "                      'momentum': 0.9,\n",
    "                      'num_epochs': 20000,\n",
    "                      'batch_size': 128,\n",
    "                      \"save_path\": \"None\",\n",
    "                      \"num_events\": 200,\n",
    "                      \"sig_eff_at\": 0.9996,\n",
    "                      \"test\":False, \"seed\": 7,\n",
    "                      \"mode\":\"classif\",\n",
    "                      \"ae\":False}\n",
    "    net, fns = build_network(inp_d, build_layers(inp_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conv_kwargs = dict(num_filters=5, filter_size=3, pad=1, nonlinearity=relu, W=HeNormal(gain=\"relu\"))\n",
    "# network = InputLayer(shape=(None,3,50,50))\n",
    "# for lay in range(3):\n",
    "#     network = batch_norm(Conv2DLayer(network, **conv_kwargs))\n",
    "#     network = MaxPool2DLayer(network, pool_size=(2,2),stride=2)\n",
    "# network = DenseLayer(network,num_units=10, nonlinearity=relu) \n",
    "# network = DenseLayer(network, num_units=3, nonlinearity=softmax)\n",
    "\n",
    "# X = T.tensor4('X')\n",
    "# Y = T.ivector('Y')\n",
    "# W = T.vector('W')\n",
    "\n",
    "# W = W / T.sum(W)\n",
    "# #network = build_layers(args)\n",
    "\n",
    "# '''write loss function equation'''\n",
    "# prediction = get_output(network, X)\n",
    "# loss = categorical_crossentropy(prediction, Y)\n",
    "# newloss = T.dot(loss.T,W) / W.shape[0]\n",
    "# loss = loss.mean()\n",
    "# #loss = loss * W\n",
    "# #loss = loss / W.shape[0]\n",
    "# val_fn = theano.function([X, Y, W], [loss, newloss, W])\n",
    "\n",
    "# x = np.random.random((64,3,50,50))\n",
    "\n",
    "# y = np.random.randint(0,3,size=(64,))\n",
    "\n",
    "# y=y.astype(\"int32\")\n",
    "\n",
    "# w = 10**3 * np.random.random((64,))\n",
    "\n",
    "# val_fn(x,y,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
