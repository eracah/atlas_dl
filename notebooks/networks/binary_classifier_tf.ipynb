{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os stuff\n",
    "import os\n",
    "import h5py as h5\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as tfk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \n",
    "    def reset(self):\n",
    "        self._epochs_completed = 0\n",
    "        self._file_index = 0\n",
    "        self._data_index = 0\n",
    "    \n",
    "    \n",
    "    def load_next_file(self):\n",
    "        with h5.File(self._filelist[self._file_index],'r') as f:\n",
    "            self._images = f['data'].value\n",
    "            self._labels = f['label'].value\n",
    "            self._normweights = f['normweight'].value\n",
    "            self._weights = f['weight'].value\n",
    "            f.close()\n",
    "        assert self._images.shape[0] == self._labels.shape[0], ('images.shape: %s labels.shape: %s' % (self._images.shape, self_.labels.shape))\n",
    "        assert self._labels.shape[0] == self._normweights.shape[0], ('labels.shape: %s normweights.shape: %s' % (self._labels.shape, self._normweights.shape))\n",
    "        \n",
    "        #set number of samples\n",
    "        self._num_examples = self._labels.shape[0]\n",
    "        \n",
    "        #create permutation\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        \n",
    "        #shuffle\n",
    "        self._images = self._images[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        self._normweights = self._normweights[perm]\n",
    "        self._weights = self._weights[perm]\n",
    "        \n",
    "        #transpose images\n",
    "        #self._images = np.transpose(self._images,(0,3,2,1))\n",
    "        #select one channel only\n",
    "        self._images = self._images[:,0:1,:,:]\n",
    "        \n",
    "        #reshape labels and weights\n",
    "        self._labels = np.reshape(self._labels,(self._labels.shape[0],1))\n",
    "        self._normweights = np.reshape(self._normweights,(self._normweights.shape[0],1))\n",
    "        self._weights = np.reshape(self._weights,(self._weights.shape[0],1))\n",
    "        \n",
    "    \n",
    "    def __init__(self, filelist):\n",
    "        \"\"\"Construct DataSet\"\"\"\n",
    "        self._num_files = len(filelist)\n",
    "        \n",
    "        assert self._num_files > 0, ('filelist is empty')\n",
    "        \n",
    "        self._filelist = filelist\n",
    "        self.reset()\n",
    "        self.load_next_file()\n",
    "\n",
    "    @property\n",
    "    def num_files(self):\n",
    "        return self._num_files\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._data_index\n",
    "        self._data_index += batch_size\n",
    "        if self._data_index > self._num_examples:\n",
    "            \n",
    "            #first, reset data_index and increase file index:\n",
    "            start=0\n",
    "            self._data_index=batch_size\n",
    "            self._file_index+=1\n",
    "            \n",
    "            #check if we are at the end of the file list\n",
    "            if self._file_index >= self._num_files:\n",
    "                #epoch is finished\n",
    "                self._epochs_completed += 1\n",
    "                #reset file index and shuffle list\n",
    "                self._file_index=0\n",
    "                np.random.shuffle(self._filelist)\n",
    "            \n",
    "            #load the next file\n",
    "            self.load_next_file()\n",
    "            assert batch_size <= self._num_examples\n",
    "        \n",
    "        end = self._data_index\n",
    "        return self._images[start:end], self._labels[start:end], self._normweights[start:end], self._weights[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSetEvan(object):\n",
    "    \n",
    "    def reset(self):\n",
    "        self._epochs_completed = 0\n",
    "        self._file_index = 0\n",
    "        self._data_index = 0\n",
    "    \n",
    "    \n",
    "    def load_next_file(self):\n",
    "        #only load a new file if there are more than one file in the list:\n",
    "        if self._num_files > 1 or not self._initialized:\n",
    "            with h5.File(self._filelist[self._file_index],'r') as f:\n",
    "                self._images = f['all_events/hist'].value\n",
    "                self._labels = f['all_events/y'].value\n",
    "                self._normweights = f['all_events/normalized_weight'].value\n",
    "                self._weights = f['all_events/weight'].value\n",
    "                f.close()\n",
    "                \n",
    "            #sanity checks\n",
    "            assert self._images.shape[0] == self._labels.shape[0], ('images.shape: %s labels.shape: %s' % (self._images.shape, self_.labels.shape))\n",
    "            assert self._labels.shape[0] == self._normweights.shape[0], ('labels.shape: %s normweights.shape: %s' % (self._labels.shape, self._normweights.shape))\n",
    "            self._initialized = True\n",
    "        \n",
    "            #set number of samples\n",
    "            self._num_examples = self._labels.shape[0]\n",
    "            \n",
    "            #transpose images\n",
    "            shape = self._images.shape\n",
    "            self._images = np.reshape(self._images,(shape[0],1,shape[1],shape[2]))\n",
    "        \n",
    "            #reshape labels and weights\n",
    "            self._labels = np.reshape(self._labels,(self._labels.shape[0],1))\n",
    "            self._normweights = np.reshape(self._normweights,(self._normweights.shape[0],1))\n",
    "            self._weights = np.reshape(self._weights,(self._weights.shape[0],1))\n",
    "            \n",
    "        #create permutation\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        #shuffle\n",
    "        self._images = self._images[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        self._normweights = self._normweights[perm]\n",
    "        self._weights = self._weights[perm]\n",
    "        \n",
    "    \n",
    "    def __init__(self, filelist):\n",
    "        \"\"\"Construct DataSet\"\"\"\n",
    "        self._num_files = len(filelist)\n",
    "        \n",
    "        assert self._num_files > 0, ('filelist is empty')\n",
    "        \n",
    "        self._filelist = filelist\n",
    "        self._initialized = False\n",
    "        self.reset()\n",
    "        self.load_next_file()\n",
    "\n",
    "    @property\n",
    "    def num_files(self):\n",
    "        return self._num_files\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._data_index\n",
    "        self._data_index += batch_size\n",
    "        if self._data_index > self._num_examples:\n",
    "            \n",
    "            #first, reset data_index and increase file index:\n",
    "            start=0\n",
    "            self._data_index=batch_size\n",
    "            self._file_index+=1\n",
    "            \n",
    "            #check if we are at the end of the file list\n",
    "            if self._file_index >= self._num_files:\n",
    "                #epoch is finished\n",
    "                self._epochs_completed += 1\n",
    "                #reset file index and shuffle list\n",
    "                self._file_index=0\n",
    "                np.random.shuffle(self._filelist)\n",
    "            \n",
    "            #load the next file\n",
    "            self.load_next_file()\n",
    "            assert batch_size <= self._num_examples\n",
    "        \n",
    "        end = self._data_index\n",
    "        return self._images[start:end], self._labels[start:end], self._normweights[start:end], self._weights[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(args):\n",
    "    \n",
    "    #find out which device to use:\n",
    "    device='/cpu:0'\n",
    "    if args['arch']=='k80':\n",
    "        device='/gpu:0'\n",
    "    \n",
    "    #define empty variables dict\n",
    "    variables={}\n",
    "    \n",
    "    #create placeholders\n",
    "    variables['images_'] = tf.placeholder(tf.float32, shape=args['input_shape'])\n",
    "    variables['keep_prob_'] = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #empty network:\n",
    "    network = []\n",
    "    \n",
    "    #input layer\n",
    "    network.append(tf.reshape(variables['images_'], [-1]+args['input_shape'][1:], name='input'))\n",
    "    \n",
    "    #get all the conv-args stuff:\n",
    "    activation=args['conv_params']['activation']\n",
    "    initializer=args['conv_params']['initializer']\n",
    "    ksize=args['conv_params']['filter_size']\n",
    "    num_filters=args['conv_params']['num_filters']\n",
    "    padding=args['conv_params']['padding']\n",
    "        \n",
    "    #conv layers:\n",
    "    prev_num_filters=1\n",
    "    for layerid in range(1,args['num_layers']+1):\n",
    "        \n",
    "        #create weight-variable\n",
    "        with tf.device(device):\n",
    "            variables['conv'+str(layerid)+'_w']=tf.Variable(initializer([ksize,ksize,prev_num_filters,num_filters]),\n",
    "                                                            name='conv'+str(layerid)+'_w')\n",
    "            prev_num_filters=num_filters\n",
    "        \n",
    "            #conv unit\n",
    "            network.append(tf.nn.conv2d(network[-1],\n",
    "                                        filter=variables['conv'+str(layerid)+'_w'],\n",
    "                                        strides=[1, 1, 1, 1], \n",
    "                                        padding=padding,\n",
    "                                        data_format=\"NCHW\",\n",
    "                                        name='conv'+str(layerid)))\n",
    "        \n",
    "        #batchnorm if desired\n",
    "        outshape=network[-1].shape[1:]\n",
    "        if args['batch_norm']:\n",
    "            with tf.device(device):\n",
    "                #mu\n",
    "                variables['bn'+str(layerid)+'_m']=tf.Variable(tf.zeros(outshape),\n",
    "                                                             name='bn'+str(layerid)+'_m')\n",
    "                #sigma\n",
    "                variables['bn'+str(layerid)+'_s']=tf.Variable(tf.ones(outshape),\n",
    "                                                             name='bn'+str(layerid)+'_s')\n",
    "                #gamma\n",
    "                variables['bn'+str(layerid)+'_g']=tf.Variable(tf.ones(outshape),\n",
    "                                                             name='bn'+str(layerid)+'_g')\n",
    "                #beta\n",
    "                variables['bn'+str(layerid)+'_b']=tf.Variable(tf.zeros(outshape),\n",
    "                                                             name='bn'+str(layerid)+'_b')\n",
    "                #add batch norm layer\n",
    "                network.append(tf.nn.batch_normalization(network[-1],\n",
    "                               mean=variables['bn'+str(layerid)+'_m'],\n",
    "                               variance=variables['bn'+str(layerid)+'_s'],\n",
    "                               offset=variables['bn'+str(layerid)+'_b'],\n",
    "                               scale=variables['bn'+str(layerid)+'_g'],\n",
    "                               variance_epsilon=1.e-4,\n",
    "                               name='bn'+str(layerid)))\n",
    "        \n",
    "        #add relu unit\n",
    "        with tf.device(device):\n",
    "            network.append(activation(network[-1]))\n",
    "        \n",
    "        #add maxpool\n",
    "        with tf.device(device):\n",
    "            network.append(tf.nn.max_pool(network[-1],\n",
    "                                          ksize=[1,1,2,2],\n",
    "                                          strides=[1,1,2,2],\n",
    "                                          padding=args['conv_params']['padding'],\n",
    "                                          data_format=\"NCHW\",\n",
    "                                          name='maxpool'+str(layerid)))\n",
    "        \n",
    "        #add dropout\n",
    "        with tf.device(device):\n",
    "            network.append(tf.nn.dropout(network[-1],\n",
    "                                         keep_prob=variables['keep_prob_'],\n",
    "                                         name='drop'+str(layerid)))\n",
    "    \n",
    "    #reshape\n",
    "    with tf.device(device):\n",
    "        network.append(tf.reshape(network[-1],shape=[-1, 8 * 8 * num_filters],name='flatten'))\n",
    "    \n",
    "    #now do the MLP\n",
    "    #fc1\n",
    "    with tf.device(device):\n",
    "        variables['fc1_w']=tf.Variable(initializer([8 * 8 * num_filters,args['num_fc_units']]),name='fc1_w')\n",
    "        variables['fc1_b']=tf.Variable(tf.zeros([args['num_fc_units']]),name='fc1_b')\n",
    "        network.append(tf.matmul(network[-1], variables['fc1_w']) + variables['fc1_b'])\n",
    "    \n",
    "    #add relu unit\n",
    "    with tf.device(device):\n",
    "        network.append(activation(network[-1]))\n",
    "    \n",
    "    #add dropout\n",
    "    with tf.device(device):\n",
    "        network.append(tf.nn.dropout(network[-1],\n",
    "                                     keep_prob=variables['keep_prob_'],\n",
    "                                     name='drop'+str(layerid)))\n",
    "    #fc2\n",
    "    with tf.device(device):\n",
    "        variables['fc2_w']=tf.Variable(initializer([args['num_fc_units'],2]),name='fc2_w')\n",
    "        variables['fc2_b']=tf.Variable(tf.zeros([2]),name='fc2_b')\n",
    "        network.append(tf.matmul(network[-1], variables['fc2_w']) + variables['fc2_b'])\n",
    "    \n",
    "    #add softmax\n",
    "    with tf.device(device):\n",
    "        network.append(tf.nn.softmax(network[-1]))\n",
    "    \n",
    "    #return the network and variables\n",
    "    return variables,network\n",
    "\n",
    "\n",
    "#build the functions\n",
    "def build_functions(variables, network):\n",
    "    #add additional variables\n",
    "    variables['labels_']=tf.placeholder(tf.int32,shape=[None,1])\n",
    "    variables['weights_']=tf.placeholder(tf.float32,shape=[None,1])\n",
    "    \n",
    "    #loss function\n",
    "    prediction = network[-1]\n",
    "    tf.add_to_collection('prediction_op', prediction)\n",
    "    \n",
    "    #compute loss, important: use unscaled version!\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(variables['labels_'],\n",
    "                                                  network[-2],\n",
    "                                                  weights=variables['weights_'])\n",
    "    \n",
    "    #compute accuracy\n",
    "    accuracy = tf.metrics.accuracy(variables['labels_'],\n",
    "                                   tf.round(prediction[:,1]),\n",
    "                                   weights=variables['weights_'],\n",
    "                                   name='accuracy')\n",
    "    \n",
    "    #compute AUC\n",
    "    auc = tf.metrics.auc(variables['labels_'],\n",
    "                         prediction[:,1],\n",
    "                         weights=variables['weights_'],\n",
    "                         num_thresholds=5000,\n",
    "                         curve='ROC',\n",
    "                         name='AUC')\n",
    "    \n",
    "    #get loss\n",
    "    return prediction, loss, accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "thorstendl",
   "language": "python",
   "name": "thorstendl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
