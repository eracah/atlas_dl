{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from os import makedirs, mkdir\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import h5py\n",
    "#from helper_fxns import suppress_stdout_stderr\n",
    "import copy\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    def __init__(self, filelist, batch_size=128, shuffle=True, keys={\"datakey\": \"data\", \"labelkey\": \"label\", \"normweightkey\":\"normweight\", \"weightkey\":\"weight\"}):\n",
    "        #keys\n",
    "        self.keys=keys\n",
    "        #batchsize and indices\n",
    "        self.batch_size=batch_size\n",
    "        #store the filelist\n",
    "        self.files=filelist\n",
    "        self.num_files=len(self.files)\n",
    "        #store the shuffle state\n",
    "        self.shuffle=shuffle\n",
    "        #file and event indices:\n",
    "        self.file_index=0\n",
    "        self.event_index=0\n",
    "        \n",
    "        #shuffle files\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.files)\n",
    "            \n",
    "        #load the initial bunch of data\n",
    "        self.load_next_file()\n",
    "    \n",
    "    \n",
    "    #iterator\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #load next file logic\n",
    "    def load_next_file(self):\n",
    "        #open file\n",
    "        f=h5py.File(self.files[self.file_index],'r')\n",
    "        #load data from file\n",
    "        self.data=f[self.keys['datakey']].value\n",
    "        self.label=f[self.keys['labelkey']].value\n",
    "        self.nweight=f[self.keys['normweightkey']].value\n",
    "        self.weight=f[self.keys['weightkey']].value\n",
    "        #close file\n",
    "        f.close()\n",
    "        \n",
    "        #datalength:\n",
    "        self.dlength=self.data.shape[0]\n",
    "        \n",
    "        #shuffle data if requested\n",
    "        if self.shuffle:\n",
    "            reindex=np.random.permutation(self.dlength)\n",
    "            self.data=self.data[reindex,:,:,:]\n",
    "            self.label=self.label[reindex]\n",
    "            self.nweight=self.nweight[reindex]\n",
    "            self.weight=self.weight[reindex]\n",
    "    \n",
    "    \n",
    "    #next function\n",
    "    def __next__(self):\n",
    "        #grep data\n",
    "        #upper index\n",
    "        upper=np.min([self.dlength,self.event_index+self.batch_size])\n",
    "        #load data\n",
    "        tmpdata=self.data[self.event_index:upper,:,:,:].astype(\"float32\")\n",
    "        tmplabel=self.label[self.event_index:upper].astype(\"int32\")\n",
    "        tmpweight=self.weight[self.event_index:upper].astype(\"float32\")\n",
    "        tmpnweight=self.nweight[self.event_index:upper].astype(\"float32\")\n",
    "        #load new file if needed:\n",
    "        if self.dlength<=(self.event_index+self.batch_size):\n",
    "            self.file_index+=1\n",
    "            \n",
    "            #check if the epoch is over\n",
    "            if self.file_index>=self.num_files:\n",
    "                #shuffle if requested\n",
    "                if self.shuffle:\n",
    "                    np.random.shuffle(self.files)\n",
    "                #reset indices\n",
    "                self.event_index=0\n",
    "                self.file_index=0\n",
    "                #prefetch next\n",
    "                self.load_next_file()\n",
    "                #stop the iteration here\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                #prefetch the file\n",
    "                self.load_next_file()\n",
    "                #fetch the missing data:\n",
    "                rlength=self.batch_size-tmpdata.shape[0]\n",
    "                tmpdata=np.concatenate([tmpdata,self.data[0:rlength,:,:,:]],axis=0)\n",
    "                tmplabel=np.concatenate([tmplabel,self.label[0:rlength]],axis=0)\n",
    "                tmpweight=np.concatenate([tmpweight,self.weight[0:rlength]],axis=0)\n",
    "                tmpnweight=np.concatenate([tmpnweight,self.nweight[0:rlength]],axis=0)\n",
    "                self.event_index=rlength\n",
    "        else:\n",
    "            self.event_index+=self.batch_size\n",
    "            \n",
    "        #return result\n",
    "        return {'hist': tmpdata, 'y': tmplabel, 'weight': tmpweight, 'norm_weight': tmpnweight}\n",
    "        \n",
    "        \n",
    "    #def iterate(self):\n",
    "    #    \n",
    "    #    #shuffle files\n",
    "    #    if self.shuffle:\n",
    "    #        self.files=np.random.shuffle(self.files)\n",
    "    #    #set step-size to batch-size\n",
    "    #    step_size=self.batch_size\n",
    "    #    \n",
    "    #    #iterate over files\n",
    "    #    for fname in self.files:\n",
    "    #        \n",
    "    #        #open file\n",
    "    #        f=h5py.File(fname,'r')\n",
    "    #        #load data from file\n",
    "    #        data=np.random.shuffle(f[keys['datakey']],axis=0).values\n",
    "    #        label=np.random.shuffle(f[keys['labelkey']],axis=0).values\n",
    "    #        nweight=np.random.shuffle(f[keys['normweightkey']],axis=0).values\n",
    "    #        weight=np.random.shuffle(f[keys['weightkey']],axis=0).values\n",
    "    #        #close file\n",
    "    #        f.close()\n",
    "    #        #datalength:\n",
    "    #        datalength=data.shape[0]\n",
    "    #        \n",
    "    #        #shuffle data if requested\n",
    "    #        if self.shuffle:\n",
    "    #            reindex=np.random.permutation(datalength)\n",
    "    #            data=data[reindex,:,:,:]\n",
    "    #            label=label[reindex]\n",
    "    #            nweight=nweight[reindex]\n",
    "    #            weight=weight[reindex]\n",
    "    #        \n",
    "    #        #iterate over entries\n",
    "    #        for event_index in range(0,datalength,step_size):\n",
    "    #            #upper index\n",
    "    #            upper=np.max([datalength,event_index+step_size])\n",
    "    #            #load data\n",
    "    #            tmpdata=data[event_index:upper,:,:,:].astype(\"float32\")\n",
    "    #            tmplabel=label[event_index:upper].astype(\"int32\")\n",
    "    #            tmpweight=weight[event_index:upper].astype(\"float32\")\n",
    "    #            tmpnweight=nweight[event_index:upper].astype(\"float32\")\n",
    "    #            \n",
    "    #            if step_size<self.batch_size:\n",
    "    #                d[\"hist\"]=np.concatenate([d[\"hist\"],tmpdata],axis=0)\n",
    "    #                d[\"y\"]=np.concatenate([d[\"y\"],tmplabel],axis=0)\n",
    "    #                d[\"weight\"]=np.concatenate([d[\"weight\"],tmpweight],axis=0)\n",
    "    #                d[\"normalized_weight\"]=np.concatenate([d[\"normalized_weight\"],tmpnweight],axis=0)\n",
    "    #            else:\n",
    "    #                d[\"hist\"]=tmpdata\n",
    "    #                d[\"y\"]=tmplabel\n",
    "    #                d[\"weight\"]=tmpweight\n",
    "    #                d[\"normalized_weight\"]=tmpnweight\n",
    "    #            \n",
    "    #            if d[\"hist\"].shape[0]<self.batch_size:\n",
    "    #               step_size=self.batch_size-d[\"hist\"].shape[0]\n",
    "    #               continue\n",
    "    #            else:\n",
    "    #                step_size=self.batch_size\n",
    "    #                yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #run_split()\n",
    "    mainpath='/global/cscratch1/sd/tkurth/atlas_dl/data_delphes'\n",
    "    trainfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_training_')]\n",
    "    validationfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_validation_')]\n",
    "    testfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_test_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
