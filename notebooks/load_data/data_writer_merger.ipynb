{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'racah'\n",
    "import numpy as np\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from os import makedirs, mkdir\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import h5py\n",
    "import copy\n",
    "import pickle\n",
    "from util import get_atlas_h5group, make_empty_dict_of_file, concat_two_dicts, \\\n",
    "                    get_data_dict_from_h5group, make_new_file, \\\n",
    "                    preprocess, preproc_file\n",
    "        \n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_files(dirpath, new_fpath):\n",
    "    files = [join(dirpath,f) for f in os.listdir(dirpath)]\n",
    "    base = make_empty_dict_of_file(files[0])\n",
    "    for fpath in files:\n",
    "        if \"RPV\" in fpath:\n",
    "            sig=True\n",
    "        else:\n",
    "            sig=False\n",
    "        print fpath\n",
    "        fgroup, h5f = get_atlas_h5group(fpath)\n",
    "        \n",
    "        d = get_data_dict_from_h5group(fgroup, sig)\n",
    "        base = concat_two_dicts(base,d)\n",
    "    \n",
    "        h5f.close()\n",
    "    print \"making new file...\"\n",
    "    make_new_file(base, new_fpath)\n",
    "    #return base\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test_files(file_path,test_name=\"test\", val_name=\"val\", train_name=\"train\",suffix=\"\", test_prop=0.2):\n",
    "    \n",
    "    def add_to_file(file_name, data_dict):\n",
    "        #overwrites file?\n",
    "        f = h5py.File(file_name, \"w\")\n",
    "        group = f.create_group(\"all_events\")\n",
    "        for k in data_dict:\n",
    "            group[k] = data_dict[k]\n",
    "        f.close()\n",
    "        \n",
    "\n",
    "    print file_path\n",
    "    h5f = h5py.File(file_path)\n",
    "    all_events = h5f[\"all_events\"]\n",
    "    num_events = all_events[\"hist\"].shape[0]\n",
    "\n",
    "    num_test = int(test_prop * num_events)\n",
    "    num_val = num_test\n",
    "    test_file_name = join(os.path.dirname(file_path),test_name + suffix + \".h5\")\n",
    "    train_file_name = join(os.path.dirname(file_path),train_name + suffix+ \".h5\")\n",
    "    val_file_name = join(os.path.dirname(file_path),val_name + suffix+ \".h5\")\n",
    "    \n",
    "    \n",
    "    inds = np.arange(num_events)\n",
    "    np.random.RandomState(11).shuffle(inds)\n",
    "    raw_data = {k:all_events[k][:] for k in all_events.keys()}\n",
    "    te_data = {k:raw_data[k][inds[:num_test]] for k in all_events.keys()}\n",
    "    val_data = {k:raw_data[k][inds[num_test:2*num_test]] for k in all_events.keys()}\n",
    "    tr_data = {k:raw_data[k][inds[2*num_test:]] for k in all_events.keys()}\n",
    "    add_to_file(test_file_name, te_data)\n",
    "    add_to_file(train_file_name, tr_data)\n",
    "    add_to_file(val_file_name, val_data)\n",
    "    return {\"tr\": train_file_name, \"te\": test_file_name, \"val\": val_file_name}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_all_files(tr_path, val_path, test_path):\n",
    "    print \"tr\"\n",
    "    tr_stats = preproc_file(tr_path)\n",
    "    print \"val\"\n",
    "    _ = preproc_file(val_path, tr_stats)\n",
    "    print \"test\"\n",
    "    _ = preproc_file(test_path, tr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    if len(sys.argv) > 2:\n",
    "        assert \"jupyter\" not in sys.argv[2], \"can't run this from a notebook!\"\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--source_path\",type=str, help=\"path where the initial files are\" )\n",
    "    ap.add_argument(\"--dest_path\", type=str, help=\"path where you want tr, val, test files\")\n",
    "    ap.add_argument(\"--suffix\", default=\"\", type=str, help=\"suffix for describing tr val test files\")\n",
    "    args = ap.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "can't run this from a notebook!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-9f46c82ab5c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msource_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdest_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msuffix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-c6a83ac70214>\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[1;34m\"jupyter\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"can't run this from a notebook!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--source_path\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"path where the initial files are\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--dest_path\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"path where you want tr, val, test files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: can't run this from a notebook!"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    source_dir = args.source_path\n",
    "    dest_dir = args.dest_path\n",
    "    suffix = args.suffix\n",
    "    \n",
    "    # merge all data in source_dir directory into a file in the dest_dir diretory\n",
    "    # called \"all_data_merged<your_suffix>.h5\"\n",
    "    merged_fpath = join(dest_dir, \"all_data_merged\" + suffix + \".h5\" )\n",
    "    merge_files(source_dir, merged_fpath)\n",
    "    \n",
    "    #split the merged file into train and test and val (60-20-20 split)\n",
    "    tr_te_val = split_train_test_files(merged_fpath,suffix=suffix)\n",
    "    \n",
    "    \n",
    "    #now normalize val and test based off of train statistics\n",
    "    normalize_all_files(tr_path=tr_te_val[\"tr\"], test_path=tr_te_val[\"te\"], val_path=tr_te_val[\"val\"])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
